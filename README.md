# Evaluating Taxonomy Free Character Role Labeling (TF-CRL) in News Stories using Large Language Models

This repository is the official implementation of Evaluating Taxonomy Free Character Role Labeling (TF-CRL) in News Stories using Large Language Models.

This contains the data, codebooks, and prompts used for the project, as well as the code to reproduce our results. Below is the breakdown of the repo structure.

Repo in progress.

<!-- <p align="center">
    <img src="https://github.com/davidghobson1/llm-story-morals/blob/main/images/moral_pipeline.png?raw=true" alt="Results Image" width="550"/>
</p>

## Repo Structure
```
.
â”œâ”€â”€ code                                             
    â”œâ”€â”€ Categorical-Automated-Validation.ipynb       # automated comparison of protagonist/antagonist/valence/protagonist type
    â”œâ”€â”€ MTurk-Analysis.ipynb                         # analyse MTurk results
    â”œâ”€â”€ automated_validation.py                      # automated comparison of human vs GPT morals
    â”œâ”€â”€ moral_clustering.py                          # cluster the morals 
|
â”œâ”€â”€ codebooks                                        
    â”œâ”€â”€ mturk_survey_template.html                   # Amazon Mechanical Turk validation template/instructions
    â”œâ”€â”€ story_morals_codebook.docx                   # codebook for human story moral annotations
|
â”œâ”€â”€ data                                              
    â”œâ”€â”€ application                                  # data for Application section of paper
        â”œâ”€â”€ fairytalez_dataset.csv
        â”œâ”€â”€ gpt_responses_fairytalex.csv
    â”œâ”€â”€ validation                                   # data for Validation section of paper
        â”œâ”€â”€ moral_annotations                        
            â”œâ”€â”€ gpt_responses_english.csv            # GPT's responses to the English validation texts
            â”œâ”€â”€ gpt_responses_mandarin.csv           # GPT's responses to the Mandarin validation texts
            â”œâ”€â”€ human_responses_english.csv          # human responses to the English validation texts
            â”œâ”€â”€ human_responses_mandarin.csv         # human responses to the Mandarin validation texts
        â”œâ”€â”€ mturk
            â”œâ”€â”€ mturk_responses.csv                  # responses from MTurk survey
        â”œâ”€â”€ validation_dataset.csv                   # text and metadata for the validation dataset 
|
â”œâ”€â”€ prompts                                            
    â”œâ”€â”€ prompts.json                                 # prompts used in our work
|
â”œâ”€â”€ requirements.txt                                  
â”œâ”€â”€ README.md
```

## Getting Started

### 1) Anaconda
```
conda create -n <myenv_name> python=3.10
conda activate <myenv_name>
conda install -c conda-forge pip  # make sure pip is installed
python -m pip install -r requirements.txt
```

### 2) Virtual Environment
```
python -m venv <myenv_name>
source <myenv_name>/bin/activate
python -m pip install -r requirements.txt
```

## Running the Code

### Notebooks

The two notebooks include the automated comparison between human and GPT responses for the protagonist, antagonist, valence, and protagonist type questions, and the inter-annotator agreement from our MTurk survery.

Both notebooks can be run as-is and use the data existing in this repo.

### Clustering

To run the moral clustering results, run the `moral_clustering.py` script as follows. (This recreates Figures 1-3 and the results from Tables 5, 6, 14-17, as well as the hyperparameter tuning).

```
python moral_clustering.py -col <column_name>
```

`column_name` can either be:
- moral: for the full-sentence morals
- moral+: positive morals
- moral-: negative morals
- text: full-text (with entity replacement already pre-applied)
- orig_text: full-text with entity replacement not pre-applied

See `python moral_clustering.py -h` for more options and explanations.

<p align="center">
    <img src="https://github.com/davidghobson1/llm-story-morals/blob/main/images/sample_moral_clustering.png?raw=true" alt="Results Image" width="400"/>
</p>

### Validation

To run the automated comparison between human and GPT responses for the moral, moral+, moral-, and central topic questions, run the `automated_validation.py` as follows. (This reproduces Tables 3 and 11).

```
python automated_validation.py -cate <category_name>
```

`category_name` can either be:
- moral: for the full-sentence morals
- moral+: positive morals
- moral-: negative morals
- central_topic: topic

Note that this script may take a while to run (~30 mins) if running on CPU because BERTScore can be quite slow.

<!-- >ðŸ“‹  Optional: include a graphic explaining your approach/main result, bibtex entry, link to demos, blog posts and tutorials -->

<!-- ## Requirements

To install requirements:

```setup
pip install -r requirements.txt
```

>ðŸ“‹  Describe how to set up the environment, e.g. pip/conda/docker commands, download datasets, etc...


## Contributing

>ðŸ“‹  Pick a licence and describe how to contribute to your code repository. 
 --> -->
